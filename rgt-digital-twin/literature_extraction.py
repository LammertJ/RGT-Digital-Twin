#
# Copyright [Aug 20, 2024] [Jacqueline Lammert, Maximilian Tschochohei]
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# This class holds an agent to extract data from scientific literature using
# the Google Gemini 1.5 Pro LLM
#

import config
import vertexai
import urllib
import warnings
import time
import json
import pandas as pd
import logging
import sys 
import ast
import os

# For access to Gemini model
from google.cloud import aiplatform
from google.oauth2 import service_account

from vertexai.generative_models import (
    GenerationConfig,
    GenerativeModel,
    HarmBlockThreshold,
    HarmCategory,
    Part,
)

def process_docs(
    filepath: str,
    prompt: str,
) -> dict:
    """
    Process docs from a filepath using a prompt and return the output as a dict
    """
    log = logging.getLogger(__name__)
    # First check if the directory exists
    try:
        if os.path.exists(filepath):
            folder = os.listdir(os.path.basename(filepath))
    except Exception as e:
        print(f"No valid path given: {e}")

    # First we initialize the dict for our output
    output = {}
    for doc in folder:
        output[doc] = None

    # Prompt the model to generate content for structured data extraction
    for doc in folder:
        if output[doc] is None:
            print(f"Processing {doc}")
            try:
                with open(f"{filepath}/{doc}", "rb") as f:
                    pdf_file = Part.from_data(data=f.read(), mime_type="application/pdf")
                response = config.literature_model.generate_content(
                    [pdf_file,prompt],
                    generation_config=config.generation_config,
                    safety_settings=config.safety_settings,
                    )
                log.info(response)
                output[doc]=response.text
            except Exception as e:
                print(f"Error in {doc}: {e}")
        else:
            print(f"Skipping {doc}")
    return output

def export_csv(
    literature: dict,
    treatments: dict,
    sep: str,
) -> str:
    """
    Process a dataframe consisting of dictionaries generated by LLMs (this contains errors!)
    To a csv that can be exported to Excel/Sheets for processing by clinicians 
    """ 
    log = logging.getLogger(__name__)
    df = pd.DataFrame()
    literature

    for study in literature:
        success = False
        try:
            # Using ast.literal_eval is inherently unsafe, but we can trust the input here
            results = ast.literal_eval(literature[study].split("python")[1].split("```")[0])
            results["Source"] = study
            results["Treatment"] = treatments[study]
            results = pd.DataFrame([results])
            df = pd.concat([df, results], ignore_index=True)
        except Exception as e:
            try:
                results = json.loads(literature[study].split("json")[1].split("```")[0])
                results["Source"] = study
                results["Treatment"] = treatments[study]
                results = pd.DataFrame([results])
                df = pd.concat([df, results], ignore_index=True)
                success = True
            except Exception as f:
                try:
                    results = ast.literal_eval(literature[study])
                    results["Source"] = study
                    results["Treatment"] = treatments[study]
                    results = pd.DataFrame([results])
                    df = pd.concat([df, results], ignore_index=True)
                    success = True
                except Exception as g:
                    print(f"Could not parse {study}; Exception: {g}")                 
                    log.info(literature[study])
                if not success:
                    print(f"Could not parse {study}; Exception: {f}")
                    log.info(literature[study])
            if not success:
                print(f"Could not parse {study}; Exception: {e}")
                log.info(literature[study])

    return df.to_csv(sep = sep)

def main():
    import logging
    logging.basicConfig(filename='literature_extraction.log',
                    filemode='a',
                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
                    datefmt='%H:%M:%S',
                    level=logging.INFO)
    
    try:
        CREDENTIALS = service_account.Credentials.from_service_account_file('credentials.json')
    except:
        print("Error: No credentials.json found in directory. Please create Google Cloud service account credentials (https://cloud.google.com/iam/docs/keys-create-delete).")
        exit()

    # Initialize the Google Cloud AI Platform
    vertexai.init(
        # your Google Cloud Project ID or number
        # environment default used is not set
        project=config.gcp_project_id,

        # the Vertex AI region you will use
        # defaults to us-central1
        location=config.gcp_region,

        # Your Service Account Credentials
        credentials=CREDENTIALS,
    )

    print(f"Folder: {sys.argv[1]}")
    print(f"Patient information: {sys.argv[2]}")
    folder = sys.argv[1]
    patient = sys.argv[2]
    summary_dict = process_docs(folder, config.literature_extraction_prompt)
    treatment_dict = process_docs(folder, config.treatment_prompt.format(patient=patient))
    csv = export_csv(summary_dict,treatment_dict,'|')
    with open("literature_extracted.csv", "w") as file:
        file.write(csv)

if __name__ == '__main__':
    main() 